<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCentos7.2%E4%B8%8ACDH5.15.1%E4%B8%AD%E5%90%AF%E7%94%A8Kerberos%2F</url>
    <content type="text"><![CDATA[Centos7.2上CDH5.15.1中启用Kerberos1.KDC服务安装及配置 下载KDC服务 1yum -y install krb5-server krb5-libs krb5-auth-dialog krb5-workstation 修改/etc/krb5.conf配置 1vi /etc/krb5.conf 12345678910111213141516171819202122232425# Configuration snippets may be placed in this directory as wellincludedir /etc/krb5.conf.d/[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false default_realm = BELLE.COM.CN[realms] BELLE.COM.CN = &#123; kdc = bi-cdh-dev-m-001 admin_server = bi-cdh-dev-m-001 &#125;[domain_realm] .bi-cdh-dev-m-001 = BELLE.COM.CN bi-cdh-dev-m-001 = BELLE.COM.CN 修改/var/kerberos/krb5kdc/kadm5.acl配置 1vi /var/kerberos/krb5kdc/kadm5.acl 1*/admin@BELLE.COM.CN * 修改/var/kerberos/krb5kdc/kdc.conf 1vi /var/kerberos/krb5kdc/kdc.conf 12345678910111213[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] BELLE.COM.CN = &#123; #master_key_type = aes256-cts max_renewable_life= 7d 0h 0m 0s acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal &#125; 创建Kerberos数据库 1kdb5_util create -r BELLE.COM.CN -s 此处需要输入Kerberos数据库的密码 创建Kerberos的管理账号 123456789kadmin.localAuthenticating as principal root/admin@BELLE.COM.CN with password.kadmin.local: addprinc admin/admin@BELLE.COM.CNWARNING: no policy specified for admin/admin@BELLE.COM.CN; defaulting to no policyEnter password for principal "admin/admin@BELLE.COM.CN": Re-enter password for principal "admin/admin@BELLE.COM.CN": Principal "admin/admin@BELLE.COM.CN" created.kadmin.local: exit 将Kerberos服务添加到自启动服务，并启动krb5kdc和kadmin服务 1234systemctl enable krb5kdcsystemctl enable kadminsystemctl start krb5kdcsystemctl start kadmin 测试Kerberos的管理员账号 12345678910[root@bi-cdh-dev-m-001 ~]# kinit admin/admin@BELLE.COM.CNPassword for admin/admin@BELLE.COM.CN: [root@bi-cdh-dev-m-001 ~]# klist Ticket cache: FILE:/tmp/krb5cc_0Default principal: admin/admin@BELLE.COM.CNValid starting Expires Service principal10/24/2018 20:54:59 10/25/2018 20:54:59 krbtgt/BELLE.COM.CN@BELLE.COM.CN renew until 10/31/2018 20:54:59 为集群安装所有Kerberos客户端，包括Cloudera Manager 1yum -y install krb5-libs krb5-workstation 在Cloudera Manager Server服务器上安装额外的包 1yum -y install openldap-clients 将KDC Server上的krb5.conf文件拷贝到所有Kerberos客户端 12scp /etc/krb5.conf bi-cdh-dev-w-001:/etc/scp /etc/krb5.conf bi-cdh-dev-w-002:/etc/ 2.CDH集群启用Kerberos 在KDC中给Cloudera Manager添加管理员账号 123456789kadmin.localAuthenticating as principal root/BELLE.COM.CN with password.kadmin.local: addprinc cloudera-scm/admin@BELLE.COM.CNWARNING: no policy specified for cloudera-scm/admin@BELLE.COM.CN; defaulting to no policyEnter password for principal "cloudera-scm/admin@BELLE.COM.CN": Re-enter password for principal "cloudera-scm/admin@BELLE.COM.CN": Principal "cloudera-scm/admin@BELLE.COM.CN" created.kadmin.local: exit 管理-安全 启用Kerberos 确保列出的所有检查项都已完成 点击“继续”，配置相关的KDC信息，Kerberos安全领域、KDC Sever主机名、KDC Admin Sever主机名 不建议让Cloudera Manager来管理krb5.conf, 点击“继续” 输入Cloudera Manager的Kerbers管理员账号，一定得和之前创建的账号一致，点击“继续” 点击“继续”启用Kerberos 勾选重启集群，点击“继续” 集群重启完成，点击“继续” 点击“继续” 点击“完成”，至此已成功启用Kerberos。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCentos7%E4%B8%8B%E9%83%A8%E7%BD%B2CDH6%2F</url>
    <content type="text"><![CDATA[Centos7下部署CDH61.安装之前官方安装步骤 CDH6与CDH5的安装步骤一致，主要包括以下四部分： 1.安全前置准备，包括安装操作系统、关闭防火墙、同步服务器时钟等； 2.外部数据库如MySQL安装 3.安装Cloudera Manager； 4.安装CDH集群； 请务必注意CDH6的安装前置条件包括如下： 外部数据库支持： MySQL 5.7或更高 MariaDB 5.5或更高 PostgreSQL 8.4或更高 Oracle 12c或更高 JDK Oracle JDK1.8，将不再支持JDK1.7 操作系统支持 RHEL 6.8或更高 RHEL 7.2或更高 SLES 12 SP2或更高 Ubuntu 16或更高 2.准备工作 ip 角色 备注 10.240.9.132 CM Server，datanode 10.240.9.165 namenode，datanode 10.240.9.170 namenode，datanode 10.240.9.171 datanode 10.240.9.172 datanode 2.1.安装依赖123456# 安装pip，安装psycopg2yum install -y mysql-devel python-devel &amp;&amp; \ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \ pip install psycopg2==2.7.5 --ignore-installed# 安装cyrus-sasl相关（Kerberos认证相关依赖）yum install -y cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi 2.2.搭建cm包yum源(10.240.9.132节点) 安装web服务器，nginx或者httpd 获取cm yum源包 1234567mkdir/data/cloudera_soft/CDH6.0.0/cmcd /data/cloudera_soft/CDH6.0.0/cm# 进入web服务器文件目录wget --recursive --no-parent --no-host-directories https://archive.cloudera.com/cm6/6.0.0/redhat7/chmod -R ugo+rX cm6cd /data/cloudera_soft/CDH6.0.0/cm/cm6/6.0.0wget https://archive.cloudera.com/cm6/6.0.0/allkeys.asc 浏览器访问http://10.240.9.132:8900/检查 12345678# 配置yum源vim /etc/yum.repos.d/cloudera-repo.repo[cloudera-repo]name=cloudera-repobaseurl=http://10.240.9.132:8900/CDH6.0.0/cm/cm6/6.0.0/redhat7/yum/enabled=1gpgcheck=0 1234567scp /etc/yum.repos.d/cloudera-repo.repo 10.240.9.165:/etc/yum.repos.d/scp /etc/yum.repos.d/cloudera-repo.repo 10.240.9.170:/etc/yum.repos.d/scp /etc/yum.repos.d/cloudera-repo.repo 10.240.9.171:/etc/yum.repos.d/scp /etc/yum.repos.d/cloudera-repo.repo 10.240.9.172:/etc/yum.repos.d/# 所有节点yum makecache 2.3.获取CDH6的parcels包(10.240.9.132节点)https://archive.cloudera.com/cdh6/6.0.0/parcels/ CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256 manifest.json 注意： 需要修改CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha256文件名为CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha 获取本地parcel包失败时可从manifest.json中获取sha值，修改CDH-6.0.0-1.cdh6.0.0.p0.537114-el7.parcel.sha的值 2.4.网络配置 修改主机名 123456# 分别在对应机器上执行hostnamectl set-hostname cdh01hostnamectl set-hostname cdh02hostnamectl set-hostname cdh03hostnamectl set-hostname cdh04hostnamectl set-hostname cdh05 配置hosts文件 (所有节点) 1234567vi /etc/hosts10.240.9.132 cdh0110.240.9.165 cdh0210.240.9.170 cdh0310.240.9.171 cdh0410.240.9.172 cdh05 配置ssh免密登录 123456789101112131415# 在cdh01上执行ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys# 添加公钥认证ssh-copy-id -i cdh01ssh-copy-id -i cdh02ssh-copy-id -i cdh03# 输入机器用户密码# 拷贝密钥对到各个机器上去scp -r ~/.ssh/ cdh01:~/scp -r ~/.ssh/ cdh02:~/scp -r ~/.ssh/ cdh03:~/ 关闭防火墙 （所有节点） 123456# 检查防火墙状态systemctl status firewalld# 关闭防火墙systemctl stop firewalld# 设置防火墙开机不自启 systemctl disable firewalld 关闭selinux（所有节点） 1234567891011# 临时关闭selinuxsetenforce 0# 设置开机不自启vi /etc/selinux/config# 修改"SELINUX=enforcing"为"SELINUX=disabled"SELINUX=disabled# 查看 SELINUX 状态：/usr/sbin/sestatus –v# SELinux status: enabled（enabled：开启；disabled：关闭） 设置swap（所有节点） 12echo vm.swappiness = 10 &gt;&gt; /etc/sysctl.confsysctl vm.swappiness=10 设置透明大页面 123456echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled# 设置到开机启动vim /etc/rc.d/rc.local# 添加如下脚本if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi if test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag fi 2.5.JDK安装（所有节点）1234567891011121314yum install -y oracle-j2sdk1.8# 配置环境变量vim /etc/profile# for jdkexport JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera/export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/binexport CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATH# 生效配置source /etc/profile# 检查javajava -version 2.6.配置ntp官方文档 安装ntp（所有节点） 1yum install -y ntp 配置 cdh01节点 12345678# cdh01主节点配置vi /etc/ntp.conf# 注释默认的server,添加以下serverserver 210.72.145.44 perfer # 中国国家受时中心server 202.112.10.36 # 1.cn.pool.ntp.orgserver 59.124.196.83 # 0.asia.pool.ntp.orgserver 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10 cdh02、cdh03节点 1234vi /etc/ntp.conf## 子节点把所有server行注释掉，添加下面一行server cdh01 所有节点 123systemctl start ntpdsystemctl enable ntpdntpstat synchronised to NTP server (10.240.9.132) at stratum 12 time correct to within 896 ms polling server every 64 s 2.7.cdh01节点安装配置MySQL 安装 12345wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum updateyum install mysql-serversystemctl start mysqld 配置 12345systemctl stop mysqldcp /etc/my.cnf /etc/my.cnf.defaultvi /etc/my.cnf# 删除原有配置，并修改为如下建议配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log#In later versions of MySQL, if you enable the binary log and do not set#a server_id, MySQL will not start. The server_id must be unique within#the replicating group.server_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=STRICT_ALL_TABLES 1234# 设置开机自启systemctl enable mysqld# 启动systemctl start mysqld 设置root密码 1234567891011121314151617/usr/bin/mysql_secure_installation[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] YNew password:Re-enter new password:Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] N[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] YAll done! 安装配置MySQL JDBC 驱动 下载 .tar.gz格式的文件地址：http://www.mysql.com/downloads/connector/j/5.1.html 1234567891011wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gztar zxvf mysql-connector-java-5.1.47.tar.gzmkdir -p /usr/share/java/cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47-bin.jar \ /usr/share/java/mysql-connector-java.jar scp -r /usr/share/java/ cdh02:/usr/share/scp -r /usr/share/java/ cdh03:/usr/share/ 创建对应服务数据库 12mysql -uroot -pmysql&gt; 1234567891011121314CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;sentry&apos;;CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;oozie&apos;;CREATE DATABASE am DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON am.* TO &apos;am&apos;@&apos;%&apos; IDENTIFIED BY &apos;am&apos;; 3.CM安装3.1.安装CM Server服务(10.240.9.132)1yum install cloudera-manager-daemons cloudera-manager-agent cloudera-manager-server 3.2.配置CDH本地parcel-repo1234# cdh01节点cp ~/CDH-6.0.0-1.cdh6.0.0.p0.537114-el7* /opt/cloudera/parcel-repocp ~/manifest.json /opt/cloudera/parcel-repochown -R cloudera-scm:cloudera-scm /opt/cloudera/ 3.3.cdh01节点初始化数据库12# cdh01节点/opt/cloudera/cm/schema/scm_prepare_database.sh mysql -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm 3.4.启动服务123456# 启动服务systemctl start cloudera-scm-server# 查看日志tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log# 日志内容包含如下信息CM启动成功INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server. 4.安装CDH服务 访问CM web界面 http://10.240.9.132:7180 用户：admin 密码：admin 安装步骤 设置swap（所有节点） 12echo vm.swappiness = 10 &gt;&gt; /etc/sysctl.confsysctl vm.swappiness=10 设置透明大页面 123456echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled# 设置到开机启动vim /etc/rc.d/rc.local# 添加如下脚本if test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled fi if test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag fi 1234# 安装pip，安装psycopg2yum install -y mysql-devel python-devel &amp;&amp; \ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python get-pip.py &amp;&amp; \ pip install psycopg2==2.7.5 --ignore-installed 配置角色]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCentos7%E5%9C%A8%E7%BA%BF%E9%83%A8%E7%BD%B2CDH5%2F</url>
    <content type="text"><![CDATA[Centos7在线部署CDH5[TOC] 1.安装之前 CDH 5 和 CM 5 的依赖和支持的版本 硬件要求指南 How-to: Select the Right Hardware for Your New Hadoop Cluster Before You Install 安装之前建议阅读如上官方安装指南 如何在Redhat7.4安装CDH5.15 2.准备工作 3台16核32G内存260G硬盘CentOS 7机器 机器规划： HostName 角色 IP cdh001 CM节点，NameNode，Active Monitor,DataNode 192.168.31.151 cdh002 DataNode、 JobTracker 192.168.31.152 cdh003 DataNode、 JobTracker 192.168.31.153 CM离线包 CM下载地址 cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz CDH离线包 CDH下载地址 CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha1 (注意下载后需要重命名为**.sha,否则安装parcel的时候会在线下载) manifest.json JDK1.8u162 下载地址 JDBC 下载地址 操作系统依赖 psmisc CM客户端和服务端启动脚本会使用pstree命令，不安装会报找不到命令 libxslt-devel libxml2-devel 不安装的话Hue安装时测试MySQL会报错 krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve Kerberos相关依赖 openldap-devel python-devel 相关开发依赖 12# 所有节点安装yum install -y psmisc libxslt-devel libxml2-devel krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve openldap-devel python-devel wget 3.CM安装本次安装使用搭建内网仓库镜像安装： 使用官方镜像仓库安装CM步骤参阅：官方在线安装指南 3.1.网络配置 修改主机名 1234# 分别在对应机器上执行hostnamectl set-hostname cdh01hostnamectl set-hostname cdh02hostnamectl set-hostname cdh03 配置hosts文件 (所有节点) 12345vi /etc/hosts192.168.31.151 cdh01192.168.31.152 cdh02192.168.31.153 cdh03 配置ssh免密登录 1234567891011121314# 在cdh01上执行ssh-keygen# 一路回车默认在~/.ssh目录下生成RSA密钥对id_rsa(私钥)，id_rsa.pub（公钥）# 添加公钥认证ssh-copy-id -i cdh01ssh-copy-id -i cdh02ssh-copy-id -i cdh03# 输入机器用户密码# 拷贝密钥对到各个机器上去scp -r ~/.ssh/ cdh01:~/scp -r ~/.ssh/ cdh02:~/scp -r ~/.ssh/ cdh03:~/ 关闭防火墙 （所有节点） 123456# 检查防火墙状态systemctl status firewalld# 关闭防火墙systemctl stop firewalld# 设置防火墙开机不自启 systemctl disable firewalld 关闭selinux（所有节点） 1234567891011# 临时关闭selinuxsetenforce 0# 设置开机不自启vi /etc/selinux/config# 修改"SELINUX=enforcing"为"SELINUX=disabled"SELINUX=disabled# 查看 SELINUX 状态：/usr/sbin/sestatus –v# SELinux status: enabled（enabled：开启；disabled：关闭） 3.2.JDK安装 官方JDK安装要求 The JDK must be 64-bit. Do not use a 32-bit JDK. The installed JDK must be a supported version as documented in CDH and Cloudera Manager Supported JDK Versions. Oracle JDK 7 is supported across all versions of Cloudera Manager 5 and CDH 5. Oracle JDK 8 is supported in C5.3.x and higher. Oracle JDK 9 is not supported in any Cloudera Manager or CDH version. The same version of the Oracle JDK must be installed on each cluster host. The JDK must be installed at /usr/java/jdk-version. Important:The RHEL-compatible and Ubuntu operating systems supported by Cloudera Enterprise all use AES-256 encryption by default for tickets. To support AES-256 bit encryption in JDK versions lower than 1.8u161, you must install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy File on all cluster and Hadoop user machines. Cloudera Manager can automatically install the policy files, or you can install them manually. For JCE Policy File installation instructions, see the README.txtfile included in the jce_policy-x.zip file. JDK 1.8u161 and higher enable unlimited strength encryption by default, and do not require policy files.On SLES platforms, do not install or try to use the IBM Java version bundled with the SLES distribution. CDH does not run correctly with that version. 根据官方指南因此本次安装选择JDK充分测试过且支持的最高版本JDK1.8u162 12345678910111213141516171819202122232425262728293031323334# 上传下载的jdk到cdh01机器上# 解压缩# 移动到/opt/java目录下mkdir /usr/java/tar zxvf jdk-8u162-linux-x64.tar.gz -C /usr/java/# 配置环境变量vi /etc/profile# 在最末尾加上# for jdkexport JAVA_HOME=/usr/java/jdk1.8.0_162export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH# 生效环境变量source /etc/profile# 检查是否生效java -version# 将jdk拷贝到其他节点上去scp -r /usr/java/ cdh02:/usr/scp -r /usr/java/ cdh03:/usr/# 拷贝环境变量到其他节点上去scp /etc/profile cdh02:/etc/scp /etc/profile cdh03:/etc/# 在其他节点检查jdk环境是否生效source /etc/profilejava -version 3.3.配置ntp官方文档 安装ntp（所有节点） 1yum install -y ntp 配置 cdh01节点 12345678# cdh01主节点配置vi /etc/ntp.conf# 注释默认的server,添加以下serverserver 210.72.145.44 perfer # 中国国家受时中心server 202.112.10.36 # 1.cn.pool.ntp.orgserver 59.124.196.83 # 0.asia.pool.ntp.orgserver 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10 cdh02、cdh03节点 1234vi /etc/ntp.conf## 子节点把所有server行注释掉，添加下面一行server cdh01 所有节点 123systemctl start ntpdsystemctl enable ntpdntpstat synchronised to NTP server (192.168.31.151) at stratum 12 time correct to within 909 ms polling server every 64 s 3.4.cdh01节点安装配置MySQL 安装 12345wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum updateyum install mysql-serversystemctl start mysqld 配置 12345systemctl stop mysqldcp /etc/my.cnf /etc/my.cnf.defaultvi /etc/my.cnf# 删除原有配置，并修改为如下建议配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log#In later versions of MySQL, if you enable the binary log and do not set#a server_id, MySQL will not start. The server_id must be unique within#the replicating group.server_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=STRICT_ALL_TABLES 1234# 设置开机自启systemctl enable mysqld# 启动systemctl start mysqld 设置root密码 1234567891011121314151617/usr/bin/mysql_secure_installation[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] YNew password:Re-enter new password:Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] N[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] YAll done! 安装配置MySQL JDBC 驱动 下载 .tar.gz格式的文件地址：http://www.mysql.com/downloads/connector/j/5.1.html 1234567891011wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gztar zxvf mysql-connector-java-5.1.47.tar.gzmkdir -p /usr/share/java/cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47-bin.jar \ /usr/share/java/mysql-connector-java.jar scp -r /usr/share/java/ cdh02:/usr/share/scp -r /usr/share/java/ cdh03:/usr/share/ 创建对应服务数据库 12mysql -uroot -pmysql&gt; 1234567891011121314CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON hue.* TO 'hue'@'%' IDENTIFIED BY 'hue';CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO 'hive'@'%' IDENTIFIED BY 'hive';CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON sentry.* TO 'sentry'@'%' IDENTIFIED BY 'sentry';CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON oozie.* TO 'oozie'@'%' IDENTIFIED BY 'oozie';CREATE DATABASE am DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON am.* TO 'am'@'%' IDENTIFIED BY 'am'; 3.5. 配置镜像仓库(cdh01节点) 搭建web文件服务器（长期使用：nginx、apache）（临时使用：Python web服务器） 下载镜像对应安装版本文件包（国内网络最好使用代理） Cloudera Manager 5: https://archive.cloudera.com/cm5/repo-as-tarball/ CDH 5: https://archive.cloudera.com/cdh5/repo-as-tarball/ 本次安装下载： cm5.15.1-centos7.tar.gz 和 cdh5.15.1-centos7.tar.gz 解压下载文件 12tar zxvf cm5.15.1-centos7.tar.gztar zxvf cdh5.15.1-centos7.tar.gz 长期仓库：nginx或者apache服务器 1234mv cm /var/www/htmlchmod -R ugo+rX /var/www/html/cmmv cdh /var/www/htmlchmod -R ugo+rX /var/www/html/cdh 临时使用 123python -m SimpleHTTPServer 8900Serving HTTP on 0.0.0.0 port 8900 ... 配置YUM源 1vi /etc/yum.repos.d/cloudera-repo.repo 12345[cloudera-repo]name=cloudera-repobaseurl=http://192.168.31.151:8900/cm/5enabled=1gpgcheck=0 3.6.安装CM Server服务1yum install cloudera-manager-daemons cloudera-manager-server 3.7.配置CDH本地parcel-repo12345678910# cdh01节点mkdir -p /opt/cloudera/parcel-repomv CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel manifest.json /opt/cloudera/parcel-repo/chown -R cloudera-scm:cloudera-scm /opt/cloudera/mkdir /var/lib/cloudera-scm-serverchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-server# cdh02、cdh03节点mkdir -p /opt/cloudera/parcel-repochown -R cloudera-scm:cloudera-scm /opt/cloudera/ 3.8.cdh01节点初始化数据库12# cdh01节点/usr/share/cmf/schema/scm_prepare_database.sh mysql -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm 3.9.启动服务123456# 启动服务systemctl start cloudera-scm-server# 查看日志tail -f /var/log/cloudera-scm-server/cloudera-scm-server.log# 日志内容包含如下信息CM启动成功INFO WebServerImpl:com.cloudera.server.cmf.WebServerImpl: Started Jetty server. 3.11.初始化CM 访问CM web界面 http://192.168.31.151:7180/ 用户：admin 密码：admin 配置CM 此处会报两个错误：内存交换空间设置和启用了透明大页面压缩，点击【查看详细信息】查看需要修改设置的主机 将机器设置成cloudera官方建议的配置 123456789sysctl vm.swappiness=10vi /etc/sysctl.conf# /etc/sysctl.conf加入或修改vm.swappiness配置vm.swappiness=10# 使配置生效sysctl -p 123456789101112131415161718192021echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho 'echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag' &gt;&gt; /etc/rc.localecho 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.localcat /etc/rc.local#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure# that this script will be executed during boot.touch /var/lock/subsys/localecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 修改完成重新运行检查主机检查，此时警告消除点击【完成】。 4.安装CDH服务 选择服务 配置角色 设置数据库连接 集群设置 开始安装 完成安装]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCentos7%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2%20CDH5%2F</url>
    <content type="text"><![CDATA[Centos7离线部署 CDH5[TOC] 1.安装之前 CDH 5 和 CM 5 的依赖和支持的版本 硬件要求指南 How-to: Select the Right Hardware for Your New Hadoop Cluster Before You Install 安装之前建议阅读如上官方安装指南 2.准备工作 3台16核32G内存260G硬盘CentOS 7机器 机器规划： HostName 角色 IP cdh001 CM节点，NameNode，Active Monitor,DataNode 192.168.31.151 cdh002 DataNode、 JobTracker 192.168.31.152 cdh003 DataNode、 JobTracker 192.168.31.153 CDH离线包 CDH下载地址 CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha1 (注意下载后需要重命名为**.sha,否则安装parcel的时候会在线下载) manifest.json JDK1.8u162 下载地址 JDBC 下载地址 操作系统依赖 psmisc CM客户端和服务端启动脚本会使用pstree命令，不安装会报找不到命令 libxslt-devel libxml2-devel 不安装的话Hue安装时测试MySQL会报错 krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve Kerberos相关依赖 openldap-devel python-devel 相关开发依赖 12# 所有节点安装yum install -y psmisc libxslt-devel libxml2-devel krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve openldap-devel python-devel wget 3.CM安装3.1.网络配置 修改主机名 1234# 分别在对应机器上执行hostnamectl set-hostname cdh01hostnamectl set-hostname cdh02hostnamectl set-hostname cdh03 配置hosts文件 (所有节点) 12345vi /etc/hosts192.168.31.151 cdh01192.168.31.152 cdh02192.168.31.153 cdh03 配置ssh免密登录 1234567891011121314# 在cdh01上执行ssh-keygen# 一路回车默认在~/.ssh目录下生成RSA密钥对id_rsa(私钥)，id_rsa.pub（公钥）# 添加公钥认证ssh-copy-id -i cdh01ssh-copy-id -i cdh02ssh-copy-id -i cdh03# 输入机器用户密码# 拷贝密钥对到各个机器上去scp -r ~/.ssh/ cdh01:~/scp -r ~/.ssh/ cdh02:~/scp -r ~/.ssh/ cdh03:~/ 关闭防火墙 （所有节点） 123456# 检查防火墙状态systemctl status firewalld# 关闭防火墙systemctl stop firewalld# 设置防火墙开机不自启 systemctl disable firewalld 关闭selinux（所有节点） 1234567891011# 临时关闭selinuxsetenforce 0# 设置开机不自启vi /etc/selinux/config# 修改"SELINUX=enforcing"为"SELINUX=disabled"SELINUX=disabled# 查看 SELINUX 状态：/usr/sbin/sestatus –v# SELinux status: enabled（enabled：开启；disabled：关闭） 3.2.JDK安装 官方JDK安装要求 The JDK must be 64-bit. Do not use a 32-bit JDK. The installed JDK must be a supported version as documented in CDH and Cloudera Manager Supported JDK Versions. Oracle JDK 7 is supported across all versions of Cloudera Manager 5 and CDH 5. Oracle JDK 8 is supported in C5.3.x and higher. Oracle JDK 9 is not supported in any Cloudera Manager or CDH version. The same version of the Oracle JDK must be installed on each cluster host. The JDK must be installed at /usr/java/jdk-version. Important:The RHEL-compatible and Ubuntu operating systems supported by Cloudera Enterprise all use AES-256 encryption by default for tickets. To support AES-256 bit encryption in JDK versions lower than 1.8u161, you must install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy File on all cluster and Hadoop user machines. Cloudera Manager can automatically install the policy files, or you can install them manually. For JCE Policy File installation instructions, see the README.txtfile included in the jce_policy-x.zip file. JDK 1.8u161 and higher enable unlimited strength encryption by default, and do not require policy files.On SLES platforms, do not install or try to use the IBM Java version bundled with the SLES distribution. CDH does not run correctly with that version. 根据官方指南因此本次安装选择JDK充分测试过且支持的最高版本JDK1.8u162 12345678910111213141516171819202122232425262728293031323334# 上传下载的jdk到cdh01机器上# 解压缩# 移动到/opt/java目录下mkdir /usr/java/tar zxvf jdk-8u162-linux-x64.tar.gz -C /usr/java/# 配置环境变量vi /etc/profile# 在最末尾加上# for jdkexport JAVA_HOME=/usr/java/jdk1.8.0_162export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH# 生效环境变量source /etc/profile# 检查是否生效java -version# 将jdk拷贝到其他节点上去scp -r /usr/java/ cdh02:/usr/scp -r /usr/java/ cdh03:/usr/# 拷贝环境变量到其他节点上去scp /etc/profile cdh02:/etc/scp /etc/profile cdh03:/etc/# 在其他节点检查jdk环境是否生效source /etc/profilejava -version 3.3.配置ntp官方文档 安装ntp（所有节点） 1yum install -y ntp 配置 cdh01节点 12345678# cdh01主节点配置vi /etc/ntp.conf# 注释默认的server,添加以下serverserver 210.72.145.44 perfer # 中国国家受时中心server 202.112.10.36 # 1.cn.pool.ntp.orgserver 59.124.196.83 # 0.asia.pool.ntp.orgserver 127.127.1.0 # local clockfudge 127.127.1.0 stratum 10 cdh02、cdh03节点 1234vi /etc/ntp.conf## 子节点把所有server行注释掉，添加下面一行server cdh01 所有节点 123systemctl start ntpdsystemctl enable ntpdntpstat synchronised to NTP server (192.168.31.151) at stratum 12 time correct to within 909 ms polling server every 64 s 3.4.cdh01节点安装配置MySQL 安装 12345wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum updateyum install mysql-serversystemctl start mysqld 配置 12345systemctl stop mysqldcp /etc/my.cnf /etc/my.cnf.defaultvi /etc/my.cnf# 删除原有配置，并修改为如下建议配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socktransaction-isolation = READ-COMMITTED# Disabling symbolic-links is recommended to prevent assorted security risks;# to do so, uncomment this line:symbolic-links = 0key_buffer_size = 32Mmax_allowed_packet = 32Mthread_stack = 256Kthread_cache_size = 64query_cache_limit = 8Mquery_cache_size = 64Mquery_cache_type = 1max_connections = 550#expire_logs_days = 10#max_binlog_size = 100M#log_bin should be on a disk with enough free space.#Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your#system and chown the specified folder to the mysql user.log_bin=/var/lib/mysql/mysql_binary_log#In later versions of MySQL, if you enable the binary log and do not set#a server_id, MySQL will not start. The server_id must be unique within#the replicating group.server_id=1binlog_format = mixedread_buffer_size = 2Mread_rnd_buffer_size = 16Msort_buffer_size = 8Mjoin_buffer_size = 8M# InnoDB settingsinnodb_file_per_table = 1innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 64Minnodb_buffer_pool_size = 4Ginnodb_thread_concurrency = 8innodb_flush_method = O_DIRECTinnodb_log_file_size = 512M[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidsql_mode=STRICT_ALL_TABLES 1234# 设置开机自启systemctl enable mysqld# 启动systemctl start mysqld 设置root密码 1234567891011121314151617/usr/bin/mysql_secure_installation[...]Enter current password for root (enter for none):OK, successfully used password, moving on...[...]Set root password? [Y/n] YNew password:Re-enter new password:Remove anonymous users? [Y/n] Y[...]Disallow root login remotely? [Y/n] N[...]Remove test database and access to it [Y/n] Y[...]Reload privilege tables now? [Y/n] YAll done! 安装配置MySQL JDBC 驱动 下载 .tar.gz格式的文件地址：http://www.mysql.com/downloads/connector/j/5.1.html 1234567891011wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.47.tar.gztar zxvf mysql-connector-java-5.1.47.tar.gzmkdir -p /usr/share/java/cp mysql-connector-java-5.1.47/mysql-connector-java-5.1.47-bin.jar \ /usr/share/java/mysql-connector-java.jar scp -r /usr/share/java/ cdh02:/usr/share/scp -r /usr/share/java/ cdh03:/usr/share/ 创建对应服务数据库 12mysql -uroot -pmysql&gt; 1234567891011121314CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON hue.* TO &apos;hue&apos;@&apos;%&apos; IDENTIFIED BY &apos;hue&apos;;CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON metastore.* TO &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;hive&apos;;CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON sentry.* TO &apos;sentry&apos;@&apos;%&apos; IDENTIFIED BY &apos;sentry&apos;;CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON oozie.* TO &apos;oozie&apos;@&apos;%&apos; IDENTIFIED BY &apos;oozie&apos;;CREATE DATABASE am DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;GRANT ALL ON am.* TO &apos;am&apos;@&apos;%&apos; IDENTIFIED BY &apos;am&apos;; 3.5. 创建用户 cloudera-scm （ 所有 节点）1useradd --system --home=/opt/cm-5.15.1/run/cloudera-scm-server/ --no-create-home --bash=/bin/false --comment "Cloudera SCM User" cloudera-scm 3.6.解压CM包123456789# cdh01节点mkdir -p /opt/cloudera/parcel-repotar -zxvf cloudera-manager-centos7-cm5.15.1_x86_64.tar.gz -C /opt/mv CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel.sha CDH-5.15.1-1.cdh5.15.1.p0.4-el7.parcel manifest.json /opt/cloudera/parcel-repo/chown -R cloudera-scm:cloudera-scm /opt/cm-5.15.1/chown -R cloudera-scm:cloudera-scm /opt/cloudera/mkdir /var/lib/cloudera-scm-serverchown -R cloudera-scm:cloudera-scm /var/lib/cloudera-scm-server 3.7.修改agent配置123# cdh01节点vi /opt/cm-5.15.1/etc/cloudera-scm-agent/config.iniserver_host=cdh01 3.8.拷贝CM包到所有节点123# cdh01节点scp -r /opt/cm-5.15.1/ cdh02:/opt/scp -r /opt/cm-5.15.1/ cdh03:/opt/ 123# cdh02、cdh03节点mkdir -p /opt/cloudera/parcel-repochown -R cloudera-scm:cloudera-scm /opt/cloudera/ 3.9.cdh01节点初始化数据库12# cdh01节点/opt/cm-5.15.1/share/cmf/schema/scm_prepare_database.sh mysql -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm 3.10.启动服务123456789# cdh01节点启动cm服务端/opt/cm-5.15.1/etc/init.d/cloudera-scm-server start# 查看启动日志tail -f /opt/cm-5.15.1/log/cloudera-scm-server/cloudera-scm-server.log # 查看到如下日志服务启动完成com.cloudera.server.cmf.WebServerImpl: Started Jetty server.# cdh01、cdh02、cdh03节点启动客户端/opt/cm-5.15.1/etc/init.d/cloudera-scm-agent start 3.11.初始化CM 访问CM web界面 http://192.168.31.151:7180/ 用户：admin 密码：admin 配置CM 此处会报两个错误：内存交换空间设置和启用了透明大页面压缩，点击【查看详细信息】查看需要修改设置的主机 将机器设置成cloudera官方建议的配置 123456789sysctl vm.swappiness=10vi /etc/sysctl.conf# /etc/sysctl.conf加入或修改vm.swappiness配置vm.swappiness=10# 使配置生效sysctl -p 123456789101112131415161718192021echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho 'echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag' &gt;&gt; /etc/rc.localecho 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.localcat /etc/rc.local#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure# that this script will be executed during boot.touch /var/lock/subsys/localecho never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled 修改完成重新运行检查主机检查，此时警告消除点击【完成】。 4.安装CDH服务 选择服务 配置角色 设置数据库连接 集群设置 开始安装 完成安装]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FFlume%2FFlume%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Flume笔记[TOC] 1.Flume概述​ Flume NG是一个分布式，高可用，可靠的系统，它能将不同的海量数据收集，移动并存储到一个数据存储系统中。轻量，配置简单，适用于各种日志收集，并支持Failover和负载均衡。并且它拥有非常丰富的组件。Flume NG采用的是三层架构：Agent层，Collector层和Store层，每一层均可水平拓展。其中Agent包含Source，Channel和Sink，三者组建了一个Agent。三者的职责如下所示： Source：用来消费（收集）数据源到Channel组件中 Channel：中转临时存储，保存所有Source组件信息 Sink：从Channel中读取，读取成功后会删除Channel中的信息 下图是Flume NG的架构图： 2.Flume介绍Flume用户手册：http://flume.apache.org/FlumeUserGuide.html Flume开发者手册：http://flume.apache.org/FlumeDeveloperGuide.html 2.1.Flume采集系统结构图 简单结构：单个agent采集数据 复杂结构：多级agent之间串联 高可用Flume NG集群 ： Flume的存储可以支持多种，这里只列举了HDFS和Kafka（如：存储最新的一周日志，并给Storm系统提供实时日志流） 2.2.Flume Sources参考： https://www.cnblogs.com/swordfall/p/8254271.html http://flume.apache.org/FlumeUserGuide.html 2.2.1.Flume Sources 类型官网文档上要求的属性是粗体字。 1.Avro Source: 监听Avro端口，从Avro client streams接收events。 2.Thrift Source： 监听Thrift端口和从外部Thrift client streams接收events。 3.Exec Source: Exec Source在启动时运行一个Unix命令行，并期望这过程在标准输出上连续生产数据。 4.JMS Source： JMS Source从JMS目标（如队列或者主题）读取消息。JMS应用程序应该可以与任何JMS提供程序一起工作，但是只能使用ActiveMQ进行测试。 5.Spooling Directory Source: 该source让你通过放置被提取文件在磁盘”spooling“目录下这一方式，提取数据。该source将会监控指定目录的新增文件，当新文件出现时解析event。event解析逻辑是可插入的。当一个给定文件被全部读取进channel之后，它被重命名，以标识为已完成（或者可选择deleted）。 6.Taildir Source: 注意：该source不能用于windows。 7.Twitter 1% firehose Source(试验): 8.Kafka Source: Kafka Source是Apache Kafka消费者，从Kfaka topics读取消息。如果你有多个Kafka source在跑，你可以配置它们在相同的Consumer Group，以使它们每个读取topics独特的分区。 9.NetCat TCP Source: netcat source监听一个给定的端口，然后把text文件的每一行转换成一个event。要求属性是粗体字。 10.NetCat UDP Source: netcat source监听一个给定的端口，然后把text文件的每一行转换成一个event。 11.Sequence Generator Source: 一个简单的序列生成器可以不断生成events，带有counter计数器，从0开始，以1递增，在totalEvents停止。当不能发送events到channels时会不断尝试。 12.Syslog Sources: 读取系统日志，并生成Flume events。UDP source以整条消息作为一个简单event。TCP source以新一行”n“分割的字符串作为一个新的event。 Syslog TCP Source :原始的，可靠的Syslog TCP source。 Multiport Syslog TCP Source:这是一个新的，更快的，多端口的Syslog TCP source版本。注意ports配置替代port。 Syslog UDP Source: 13.HTTP Source: source 通过HTTP POST 和 GET，接收Flume events。GET只能用于试验。HTTP requests通过 必须实现 HTTPSourceHandler接口的 ”handler“ 转换成flume events。该handler获取HttpServletRequest，然后返回一系列的flume events。 14.Stress Source: StressSource 是内部负载生成source的实现，这对于压力测试是非常有用的。它允许用户配置Event有效载荷的大小。 15.Legacy Sources: Legacy sources允许Flume 1.x agent接收来自Flume 0.9.4 agents的events。legacy source 支持Avro和Thrift RPC 连接。为了使用两个Flume 版本搭建的桥梁，你需要开始一个带有avroLegacy或者thriftLegacy source的Flume 1.x agent。0.9.4agent应该有agent Sink指向1.x agent的host/port。 Avro Legacy Source Thrift Legacy Source 16.Custom Source(自定义Source): 自定义Source是你实现Source接口。当启动Flume agent时，一个自定义source类和它依赖项必须在agent的classpath中。 17.Scrible Source: Scribe是另一种类型的提取系统。采用现有的Scribe提取系统，Flume应该使用基于Thrift的兼容传输协议的ScribeSource。 2.3.Flume Sinks参考： http://www.cnblogs.com/swordfall/p/8157766.html http://flume.apache.org/FlumeUserGuide.html 2.3.1.Flume Sinks 类型 HDFS Sink: 该sink把events写进Hadoop分布式文件系统（HDFS）。它目前支持创建文本和序列文件。它支持在两种文件类型压缩。文件可以基于数据的经过时间或者大小或者事件的数量周期性地滚动。它还通过属性（如时间戳或发生事件的机器）把数据划分为桶或区。 Hive Sink: 该sink streams 将包含分割文本或者JSON数据的events直接传送到Hive表或分区中。使用Hive 事务写events。当一系列events提交到Hive时，它们马上可以被Hive查询到。 Logger Sink: Logs event 在INFO 水平。典型用法是测试或者调试。 Avro Sink: Flume events发送到sink，转换为Avro events，并发送到配置好的hostname/port。从配置好的channel按照配置好的批量大小批量获取events。 Thrift Sink: Flume events发送到sink，转换为Thrift events，并发送到配置好的hostname/port。从配置好的channel按照配置好的批量大小批量获取events。 IRC Sink: IRC sink从链接的channel获取消息和推送消息到配置的IRC目的地。 File Roll Sink: 在本地文件系统存储events。 Null Sink: 当接收到channel时丢弃所有events。 HBaseSinks: 该sink写数据到HBase。 AsyncHBaseSink: 该sink采用异步模式写数据到HBase。 MorphlineSolrSink: 该sink从Flume events提取数据并转换，在Apache Solr 服务端实时加载，Apache Solr servers为最终用户或者搜索应用程序提供查询服务。 ElasticSearchSink: 该sink写数据到elasticsearch集群。 Kite Dataset Sink: 试验sink写event到Kite Dataset。 Kafka Sink: Flume Sink实现可以导出数据到一个Kafka topic。 HTTP Sink: 该sink将会从channel获取events，并使用HTTP POST请求发送这些events到远程服务。event 内容作为POST body发送。 Custom Sink: 自定义sink是你实现Sink接口。当启动Flume agent时，一个自定义sink类和它依赖项必须在agent的classpath中。 2.3.2.Flume Sink Processors​ Sinks groups 允许用户把多个sinks分组汇入到一个实体中。Sink processors可以用于在组内所有sinks提供负载平衡，或者在暂时失败的情况下实现从一个sink到另一个sink的故障转移。 Default Sink Processor ​ 默认sink processor只接收一个简单sink。用户没有强制去为单个sinks创建processor(sink group)。相反，用户可以按照用户指南上解释的source - channel - sink 模式。 Failover Sink Processor ​ Failover Sink Processor 维护sinks的优先列表，保证当有可用的events将会被处理。 Load balancing Sink Processor ​ Load balancing sink processor 提供了对多个sinks进行负载平衡的能力。 2.3.3.Event Serializersfile_roll sink和hdfs sink都支持EventSerializer接口。下面提供了Flume附带的EventSerializers的细节。 Body Text Serializer 别名：text。拦截器将event的主体写入输出流，而没进行任何的转换或者修改。event header被忽略。配置选项： “Flume Event” Avro Event Serializer 别名：avro_event。 拦截器将Flume events序列化成一个Avro容器文件。所使用的模式与Avro RPC机制中用于Flume events的模式相同。 该serializer继承自AbstractAvroEventSerializer类。 Avro Event Serializer 别名：该serializer没有别名，必须指定使用的类名。 2.4. Flume Channel参考： http://www.cnblogs.com/swordfall/p/8169554.html http://flume.apache.org/FlumeUserGuide.html 2.4.1. Flume Channel 类型 Memory Channel（内存Channels） ​ events存储在配置最大大小的内存队列中。对于流量较高和由于agent故障而准备丢失数据的流程来说，这是一个理想的选择。 JDBC Channel ​ events存储在持久化存储库中（其背后是一个数据库）。JDBC channel目前支持嵌入式Derby。这是一个持续的channel，对于可恢复性非常重要的流程来说是理想的选择。 Kafka Channel events存储在Kafka集群中。Kafka提供高可用性和高可靠性，所以当agent或者kafka broker 崩溃时，events能马上被其他sinks可用。 Kafka channel可以被多个场景使用： Flume source和sink - 它为events提供可靠和高可用的channel Flume source和interceptor，但是没sink - 它允许写Flume evnets到Kafka topic Flume sink，但是没source - 这是一种低延迟，容错的方式从Kafka发送events到Flume sinks 例如 HDFS, HBase或者Solr File Channel Spillable Memory Channel events存储在内存队列和磁盘中。该channel目前正在试验中，不要求在生产环境中使用。 Pseudo Transaction Channel 注意：Pseudo Transaction Channel只用于单元测试，不用于生产环境使用。 Custom Channel ​ 自定义channel是你实现Channel接口。当Flume agent启动时，一个自定义channel类和它依赖项必须包含在agent的classpath。 2.4.2.Flume Channel Selectors如果类型没有指定，那么默认“replicating”。 Replicating Channel Selector(default) (复制channel选择器) Multiplexing Channel Selector （多路复用Channel选择器） Custom Channel Selector (自定义Channel选择器) 一个自定义channel选择器（selector）是实现ChannelSelector的接口。当Flume agent启动时，一个自定义channel selector类和它依赖项必须包含在agent的classpath。 2.5.Flume Interceptors（拦截器）参考： http://www.cnblogs.com/swordfall/p/8207880.html http://flume.apache.org/FlumeUserGuide.html ​ Flume有能力修改/删除流程中的events。这是在拦截器（interceptor）的帮助下完成的。拦截器（Interceptors）是实现org.apache.flume.interceptor.Interceptor接口的类。一个interceptor可以根据interceptor的开发者选择的任何标准来修改，甚至放弃events。这个可以通过在配置中指定一系列interceptor生成类名来实现。Interceptors在source配置中被指定作为空白分隔符列表。如果interceptor需要放弃events，它不会在它需要返回的列表中返回该events。如果interceptor放弃全部events，然后它返回一个空列表。简单示例： 12345678910a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.interceptors = i1 i2a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Buildera1.sources.r1.interceptors.i1.preserveExisting = falsea1.sources.r1.interceptors.i1.hostHeader = hostnamea1.sources.r1.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Buildera1.sinks.k1.filePrefix = FlumeData.%&#123;CollectorHost&#125;.%Y-%m-%da1.sinks.k1.channel = c1 ​ 注意：该interceptor构建是被传递给type配置属性。interceptors本身是可配置的，并且可以像传递给其他可配置组件一样传递配置值。在上述示例中，events先传递到HostInterceptor，并且events被HostInterceptor返回，然后独自传递到TimestampInterceptor。你可以指定完全限定的类名称或者别名 timestamp。如果你有多个收集器写到同一个HDFS路径，然后你也可以使用HostInterceptor。 2.5.1.Flume Interceptors 种类 Timestamp Interceptor 该interceptor向event headers插入秒级时间，当event被处理时。该interceptor插入一个带有关键timestamp（或者由header属性指定）的header，其值是相关的timestamp。该interceptors可以保留一个已存在timestamp，如果它已经在配置中预先配置。 Host Interceptor 该interceptor插入运行agent的host的hostname或者IP地址。它根据配置插入带有密钥host或配置密钥（其值为host的hostname或IP地址）的header。 Static Interceptor 静态interceptor运行用户给所有events添加一个带有静态值的静态header。 Remove Header Interceptor 该interceptor通过移除一个或多个headers来操作Flume event headers。它可以移除一个静态定义的header，基于规则表达式的headers或者在一个列表中的headers。如果这些没有定义，或者如果没有header匹配到标准，Flume events将不会修改。 注意：如果只有一个header需要移除，通过名字指定它可以提供比其他两种方法更好的性能。 UUID Interceptor 该interceptor在被拦截的所有事件上设置一个通用唯一的标识符。 Morphline Interceptor 该interceptor通过morphline配置文件过滤events，该配置文件定义了一条从一个命令到另一个命令管道记录的转换命令链。例如，morphline可以忽略某些events，或者通过基于正则表达式的模式匹配来改变或者插入某些event headers，或者它可以通过Apache Tika自动检测和设置一个MIME类型在被拦截的events上。 Search and Replace Interceptor 该interceptor提供了基于Java正则表达式的简单的基于字符串的search-and-replace功能。回溯/组捕获也是可用的。这个interceptor使用与Java Matcher.replaceAll()方法相同的规则。 Regex Flitering Interceptor 该拦截器通过将event正文解释为文本并将文本与配置的正则表达式进行匹配来选择性地过滤events。 Regex Extractor Interceptor 此interceptor使用指定的正则表达式提取正则表达式匹配组，并将匹配组附加为event的headers。 该serializers用于将匹配映射到header名称和格式化的header值；默认的，你只需要指定header名称和默认org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer将会被使用。这个serializer只是将匹配映射到指定的header名称，并传递通过由正则表达式提取的值。 3.Flume安装3.1.JDK下载安装 下载对应版本JavaSDK，下载地址：http://www.oracle.com/technetwork/java/javase/downloads/index.html 解压jdk到/usr/local目录下 配置环境变量 12345678910vim /etc/profile# 在最后面添加如下几行# for jdkexport JAVA_HOME=/usr/local/jdk1.8.0_171export PATH=$PATH:$JAVA_HOME/bin# 使环境变量生效source /etc/profile 3.2.Flume下载安装 下载二进制包，下载地址：http://flume.apache.org/download.html 下载校验文件，文件名*.asc, 下载地址：http://flume.apache.org/download.html 下载KEYS，地址：http://www.apache.org/dist/flume/KEYS 校验下载的二进制包 12345678gpg --import KEYSgpg --verify apache-flume-1.8.0-bin.tar.gz.ascgpg: Signature made Fri 15 Sep 2017 09:04:39 PM CST using RSA key ID 4199ACFFgpg: Good signature from "Denes Arvay &lt;denes@apache.org&gt;"gpg: WARNING: This key is not certified with a trusted signature!gpg: There is no indication that the signature belongs to the owner.Primary key fingerprint: 4CF1 5CF1 525F CE29 F66C 08FA 302C B2A8 4199 ACFF 解压二进制包到/usr/local目录下，目录结构如下 12345├── bin├── conf├── lib├── logs└── tools 配置Flume 123456cp conf/flume-env.sh.template conf/flume-env.shvim conf/flume-env.sh# 打开这两行注释，改成实际需要的参数export JAVA_HOME=/usr/local/jdk1.8.0_171export JAVA_OPTS="-Xms1024m -Xmx1024m -Dcom.sun.management.jmxremote" 4.高可用Flume NG集群搭建 ：参考： https://www.cnblogs.com/smartloli/p/4468708.html 4.1节点分配 名称 Host 角色 Agent1 epp-nginx-001v Web Server Agent2 epp-nginx-002v Web Server Collector1 bi-slave1 AgentMstr1 Collector2 bi-slave2 AgentMstr2 Agent1，Agent2数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。要把所有的日志都收集到一个集群中存储。 4.2.配置 Agent1和Agent2的配置相同： flume-client.conf 1234567891011121314151617181920212223242526272829303132a1.sources = r1a1.sinks = k1 k2a1.channels = c1a1.sources.r1.channels = c1a1.sources.r1.type = taildira1.sources.r1.positionFile = /data/logs/flume/taildir_position.jsona1.sources.r1.filegroups = f1a1.sources.r1.filegroups.f1 = /usr/local/nginx/logs/ecloud.belle.cn.access.loga1.sources.r1.fileHeader = truea1.sinks.k1.channel = c1a1.sinkgroups = g1 a1.sinkgroups.g1.sinks = k1 k2a1.sinks.k1.type = avroa1.sinks.k1.hostname = bi-slave1a1.sinks.k1.port = 10000a1.sinks.k1.batch-size = 1000a1.sinks.k2.channel = c1a1.sinks.k2.type = avroa1.sinks.k2.hostname = bi-slave2a1.sinks.k2.port = 10000a1.sinks.k2.batch-size = 1000a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 10a1.sinkgroups.g1.processor.priority.k2 = 1a1.sinkgroups.g1.processor.maxpenalty = 10000a1.channels.c1.type = memorya1.channels.c1.capacity = 5000a1.channels.c1.transactionCapacity = 5000 Collector1和Collector2下沉到使用kerboros安全认证的CDH集群HDFS中配置 bi-slave1: nginxlog.sources.r1.bind = bi-slave1 bi-slave1: nginxlog.sources.r1.bind = bi-slave2 12345678910111213141516171819202122232425262728nginxlog.sinks = k1nginxlog.channels = c1nginxlog.sources.r1.channels = c1nginxlog.sources.r1.type = avronginxlog.sources.r1.channels = c1nginxlog.sources.r1.bind = bi-slave1nginxlog.sources.r1.port = 10000nginxlog.sinks.k1.channel = c1nginxlog.sinks.k1.type = hdfsnginxlog.sinks.k1.hdfs.kerberosPrincipal = hdfs/bi-slave1nginxlog.sinks.k1.hdfs.kerberosKeytab = /usr/flume-keytab/hdfs.keytabnginxlog.sinks.k1.hdfs.path = /flume/events/%y-%m-%dnginxlog.sinks.k1.hdfs.filePrefix = eventsnginxlog.sinks.k1.hdfs.round = truenginxlog.sinks.k1.hdfs.roundValue = 24nginxlog.sinks.k1.hdfs.roundUnit = hournginxlog.sinks.k1.hdfs.rollInterval = 600nginxlog.sinks.k1.hdfs.rollSize = 268435456nginxlog.sinks.k1.hdfs.rollCount = 2000nginxlog.sinks.k1.hdfs.batchSize = 1000nginxlog.sinks.k1.hdfs.useLocalTimeStamp = truenginxlog.sinks.k1.hdfs.fileType = DataStreamnginxlog.channels.c1.type = memorynginxlog.channels.c1.capacity = 5000nginxlog.channels.c1.transactionCapacity = 5000 Collector1和Collector2下沉到使用kerboros安全认证的CDH集群kafka中配置 12345678910111213141516171819202122nginxlog.sources = r1nginxlog.sinks = k1nginxlog.channels = c1nginxlog.sources.r1.channels = c1nginxlog.sources.r1.type = avronginxlog.sources.r1.channels = c1nginxlog.sources.r1.bind = bi-slave1nginxlog.sources.r1.port = 10000nginxlog.sinks.k1.channel = c1nginxlog.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinknginxlog.sinks.k1.kafka.producer.security.protocol = SASL_PLAINTEXTnginxlog.sinks.k1.kafka.producer.sasl.kerberos.service.name = kafkanginxlog.sinks.k1.kafka.producer.group.id = flume-producernginxlog.sinks.k1.kafka.topic = nginxlognginxlog.sinks.k1.kafka.bootstrap.servers = 172.17.194.17:9092,172.17.194.18:9092,172.17.194.20:9092nginxlog.sinks.k1.kafka.flumeBatchSize = 1000nginxlog.channels.c1.type = memorynginxlog.channels.c1.capacity = 5000nginxlog.channels.c1.transactionCapacity = 5000 4.3.启动 在Agent节点上启动命令如下所示： 1bin/flume-ng agent -c conf -f conf/flume-client.conf -n a1 -Dflume.root.logger=INFO,console &gt;&gt; /data/logs/flume/flume_client.log 2&gt;&amp;1 &amp; CDH集群上修改bi-slave1和bi-slave2的配置后重启两个节点 4.4.Failover测试在CDH集群中停止bi-slave1的Flume NG服务，查看bi-slave1、bi-slave2的日志，并检查数据的完整性，结果：bi-slave1服务结束后，flume采集的下沉任务到bi-slave2上了，且数据衔接上了。 bi-slave1恢复服务后，flume采集的下沉任务又回到了bi-slave1上了。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCDH6.0.0update6.1.0%2FCDH6.0.0%E4%B8%8D%E5%81%9C%E6%AD%A2%E6%9C%8D%E5%8A%A1%E5%8D%87%E7%BA%A76.1.0%2F</url>
    <content type="text"><![CDATA[CDH6.0.0升级6.1.0步骤官方文档 参考文档 [TOC] 1.CM升级1.1.升级概述 升级CDH集群，可以通过Cloudera Manager使用Cloudera parcels升级整个集群，也可以在所有群集主机使用基于RPM包的命令手动安装软件，然后Cloudera Manager完成服务升级。 可以不同时升级Cloudera Manager和CDH，但Cloudera Manager和CDH的版本必须兼容，Cloudera Manager的主要+次要版本等于或高于CDH的主要+次要版本。 1.2.下载文件CM6.1.0 北京IDC内网地址 1.3.备份官方文档 1.3.1.收集信息 操作系统信息 1lsb_release -a 查看数据库配置信息 1cat /etc/cloudera-scm-server/db.properties 123456789101112# cat /etc/cloudera-scm-server/db.properties# Auto-generated by scm_prepare_database.sh on Thu Dec 6 18:42:08 CST 2018## For information describing how to configure the Cloudera Manager Server# to connect to databases, see the "Cloudera Manager Installation Guide."#com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=localhostcom.cloudera.cmf.db.name=scmcom.cloudera.cmf.db.user=scmcom.cloudera.cmf.db.setupType=EXTERNALcom.cloudera.cmf.db.password=scm 1.3.2.备份CM Agent 创建备份目录 （在所有主机上执行） 12345export CM_BACKUP_DIR="`date +%F`-CM6.0.0"echo $CM_BACKUP_DIRmkdir backupcd backupmkdir -p $CM_BACKUP_DIR 备份CM Agent当前运行状态（在所有主机上执行） 1tar -cf $CM_BACKUP_DIR/cloudera-scm-agent.tar --exclude=*.sock /etc/cloudera-scm-agent /etc/default/cloudera-scm-agent /var/run/cloudera-scm-agent /var/lib/cloudera-scm-agent The tar commands in the steps below may return the following message. It is safe to ignore thismessage: 12&gt; tar: Removing leading `/&apos; from member names&gt; 备份yum源（在所有主机上执行） 1tar -cf $CM_BACKUP_DIR/repository.tar /etc/yum.repos.d 1.3.3.备份Cloudera Management Service 在Service Monitor role 机器上执行 12cp -rp /var/lib/cloudera-service-monitor /var/lib/cloudera-service-monitor-`date +%F`-CM6.0.0## 如果自定义了目录需要修改为自定义的目录 在Host Monitor role机器上执行 1cp -rp /var/lib/cloudera-host-monitor /var/lib/cloudera-host-monitor-`date +%F`-CM6.0.0 在 Event Server role机器上执行 1cp -rp /var/lib/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver-`date +%F`-CM6.0.0 1.3.4.停止CM相关服务 停止Cloudera Management Service 登录CM管理界面操作 停止Cloudera Manager Server （CM机器上执行） 1systemctl stop cloudera-scm-server 1.3.5.备份CM数据库（mysql机器上执行）1mysqldump --databases scm --host=localhost --port=3306 -u scm -p &gt; $HOME/scm-backup-`date +%F`-CM6.0.0.sql 1.3.6.备份CM其他相关服务的数据库（mysql机器上执行） 1mysqldump --databases am --host=localhost --port=3306 -u am -p &gt; $HOME/am-backup-`date +%F`-CM6.0.0.sql 1.3.7.备份Cloudera Manager Server（CM机器上执行）1tar -cf $CM_BACKUP_DIR/cloudera-scm-server.tar /etc/cloudera-scm-server /etc/default/cloudera-scm-server 1.4.升级Cloudera Manager Server 删除yum源 1rm /etc/yum.repos.d/cloudera*manager.repo* 添加新的yum源 1vim /etc/yum.repos.d/cloudera-manager.repo 12345[cloudera-repo]name=cloudera-repobaseurl=http://10.240.9.132:8900/CDH6.1.0/cm/cm6/6.1.0/redhat7/yum/enabled=1gpgcheck=0 123yum clean allrm -rf /var/cache/yumyum repolist 停止cloudera-scm-agent服务 1systemctl stop cloudera-scm-agent 升级CM 12yum clean allyum upgrade cloudera-manager-server cloudera-manager-daemons cloudera-manager-agent 如果弹出更新配置文件选项选择N You might be prompted about your configuration file version: 12345678910&gt; Configuration file &apos;/etc/cloudera-scm-agent/config.ini&apos;&gt; ==&gt; Modified (by you or by a script) since installation.&gt; ==&gt; Package distributor has shipped an updated version.&gt; What would you like to do about it ? Your options are:&gt; Y or I : install the package maintainer&apos;s version&gt; N or O : keep your currently-installed version&gt; D : show the differences between the versions&gt; Z : start a shell to examine the situation&gt; The default action is to keep your current version.&gt; You may receive a similar prompt for /etc/cloudera-scm-server/db.properties. Answer N to both prompts. You may be prompted to accept the GPG key. Answer y. 123456&gt; Retrieving key from https://archive.cloudera.com/.../cm/RPM-GPG-KEY-cloudera&gt; Importing GPG key ...&gt; Userid : &quot;Yum Maintainer &lt;webmaster@cloudera.com&gt;&quot;&gt; Fingerprint: ...&gt; From : https://archive.cloudera.com/.../RPM-GPG-KEY-cloudera&gt; 检查安装的包是否正确 1rpm -qa 'cloudera-manager-*' 123cloudera-manager-server-6.1.0-769885.el7.x86_64cloudera-manager-daemons-6.1.0-769885.el7.x86_64cloudera-manager-agent-6.1.0-769885.el7.x86_64 启动服务 12systemctl start cloudera-scm-agentsystemctl start cloudera-scm-server 检查日志 12tail -f /var/log/cloudera-scm-server/cloudera-scm-server.logtail -f /var/log/cloudera-scm-agent/cloudera-scm-agent.log 登录CM管理平台http://10.240.9.132:7180自动运行升级向导 输入升级源 http://10.240.9.132:8900/CDH6.1.0/cm/cm6/6.1.0/ Cloudera Manager在升级后报告过时的配置，请重新启动集群服务并重新部署客户端配置 检查升级结果 查看Cloudera Manager版本 验证Agent是否向Cloudera Manager发送心跳 检查所有主机 查看检查结果 检查集群历史监控数据 至此CM升级成功。 2.CDH升级官方文档 2.1.准备安装包官方CDH6.1.0下载地址 北京IDC内网地址 2.2.服务检查 HDFS检查 123# 登录任意一个节点，检查HDFS是否有错误，有错误修改后继续sudo -u hdfs hdfs fsck / -includeSnapshotssudo -u hdfs hdfs dfsadmin -report HBase检查 12# 登录任意一个DataNode节点，执行如下命令检查，有错误的话，修正错误后继续sudo -u hdfs hbase hbck KUDU检查 123# 执行如下命令检查，有错误的话，修正错误后继续# kudu kudu cluster ksck kudu_master_host1,ksck kudu_master_host2sudo -u kudu kudu cluster ksck bjds-bi-odp-prd-10-240-9-132-belle.lan 2.3.进入维护模式1.CM首选选择进入维护模式 2.升级预处理 Ensure that new applications, such as MapReduce or Spark applications, will not be submitted to the cluster until the upgrade is complete. In the Cloudera Manager Admin Console, navigate to the YARN service for the cluster you are upgrading. On the Instances tab, select all the NodeManager roles. This can be done by filtering for the roles under Role Type. Click Actions for Selected (number) &gt; Decommission.If the cluster runs CDH 5.9 or higher and is managed by Cloudera Manager 5.9 or higher, and you configured graceful decommission, the countdown for the timeout starts. A Graceful Decommission provides a timeout before starting the decommission process. The timeout creates a window of time to drain already running workloads from the system and allow them to run to completion. Search for the Node Manager Graceful Decommission Timeout field on the Configuration tab for the YARN service, and set the property to a value greater than 0 to create a timeout. Wait for the decommissioning to complete. The NodeManager State is Stopped and the Commission State is Decommissioned when decommissioning completes for each NodeManager. With all the NodeManagers still selected, click Actions for Selected (number) &gt; Recommission. Important: Do not start the NodeManagers. 2.4.备份数据库12345678# 备份huemysqldump --databases hue --host=localhost --port=3306 -u hue -p &gt; hue-backup-`date +%F`-CDH6.0.0.sql# 备份hive元数据mysqldump --databases metastore --host=localhost --port=3306 -u hive -p &gt; metastore-backup-`date +%F`-CDH6.0.0.sql# 备份ooziemysqldump --databases oozie --host=localhost --port=3306 -u oozie -p &gt; oozie-backup-`date +%F`-CDH6.0.0.sql# 备份sentrymysqldump --databases sentry --host=localhost --port=3306 -u sentry -p &gt; sentry-backup-`date +%F`-CDH6.0.0.sql 2.5.备份ZooKeeper(所有节点)1cp -rp /var/lib/zookeeper/ /var/lib/zookeeper-backup-`date +%F`CM6.1.0-CDH6.0.0 2.6.备份数据 启用了HA的HDFS需要登录所有的JournalNode节点备份 1cp -rp /data/dfs/jn /data/dfs/jn-CM6.1.0-CDH6.0.0 备份所有的NameNode节点运行状态 12345mkdir -p /etc/hadoop/conf.rollback.namenodecd /var/run/cloudera-scm-agent/process/ &amp;&amp; cd `ls -t1 | grep -e "-NAMENODE\$" | head -1`cp -rp * /etc/hadoop/conf.rollback.namenode/rm -rf /etc/hadoop/conf.rollback.namenode/log4j.propertiescp -rp /etc/hadoop/conf.cloudera.hdfs/log4j.properties /etc/hadoop/conf.rollback.namenode/ 备份NameNode的元数据（所有namenode主机） 1tar -czvf $CM_BACKUP_DIR/nn_backup.tar.gz /data/dfs/nn/ 备份所有的DataNode运行状态（所有运行DataNode主机） 12345mkdir -p /etc/hadoop/conf.rollback.datanode/cd /var/run/cloudera-scm-agent/process/ &amp;&amp; cd `ls -t1 | grep -e "-DATANODE\$" | head -1`cp -rp * /etc/hadoop/conf.rollback.datanode/rm -rf /etc/hadoop/conf.rollback.datanode/log4j.propertiescp -rp /etc/hadoop/conf.cloudera.hdfs/log4j.properties /etc/hadoop/conf.rollback.datanode/ 备份Hue，所有的Hue Server主机 123# 在所有的Hue Server角色上运行mkdir -p /opt/cloudera/parcels_backupcp -rp /opt/cloudera/parcels/CDH/lib/hue/app.reg /opt/cloudera/parcels_backup/app.reg-CM6.1.0-CDH6.0.0 2.7.向集群中添加新版的CDH存储库 http://10.240.9.132:8900/CDH6.1.0/cdh/ 2.8.运行升级向导开始升级 2.9.功能验证 运行一个MR任务 1sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 100 Hue测试 检查原有数据 CDH6.0.0的数据 升级CDH6.1.0后的数据 创建数据库 创建表和插入数据 2.10.最终化元数据升级在最终化元数据之前，进行几天甚至几周的运行观察集群是否正常，在发现所有任务都没有任何异常情况后，再进行最终化元数据操作。一旦进行最终化元数据之后，就不能回滚到老的版本了，除非有数据备份。对NameNode的主备节点都执行最终化元数据升级操作： 3.CDH回滚官方文档 参考文档 3.1.回滚限制 HDFS - 不能对进行过最终化元数据升级（2.10步骤）的HDFS集群进行回滚。 集群升级后的配置变化将不会被保留 - 包括升级后的角色、服务的变更。Cloudera 的建议是最终化元数据升级后再进行集群角色和服务的变更。 HBase – If your cluster is configured to use HBase replication, data written to HBase after the upgrade might not be replicated to peers when you start your rollback. This topic does not describe how to determine which, if any, peers have the replicated data and how to roll back that data. For more information about HBase replication, see HBase Replication. Sqoop 1 – Because of the changes introduced in Sqoop metastore logic, the metastore database that is created by the CDH 6.x version of Sqoop cannot be used by earlier versions. Sqoop 2 – As described in the upgrade process, Sqoop2 had to be stopped and deleted before the upgrade process and therefore will not be available after the rollback. Kafka – Once the Kafka log format and protocol version configurations (the inter.broker.protocol.version and log.message.format.version properties) are set to the new version (or left blank, which means to use the latest version), Kafka rollback is not possible. 3.2.停止服务 3.3.回滚CDH 回滚Parcel 点击激活会弹出对话框，选择第二个选项，仅激活，如果不小心选择了第一个选项，开始下面步骤前，先停止服务。 还原ZooKeeper（所有主机） 12rm -fr /var/lib/zookeeper/cp -rp /var/lib/zookeeper-backup-`date +%F`CM6.1.0-CDH6.0.0 /var/lib/zookeeper 通过CM启动zookeeper 回滚HDFS 对于开启了HA的HDFS回滚Journal Nodes 登录每个Journal Node节点主机，运行下面命令 1rm -fr /data/dfs/jn/nameservice1/current/* 1cp -rp /data/dfs/jn-CM6.1.0-CDH6.0.0/nameservice1/current/* /data/dfs/jn/nameservice1/current/ 通过CM启动JournalNodes角色 回滚 NameNode（NameNode角色主机执行） 编辑/etc/hadoop/conf.rollback.namenode/hdfs-site.xml文件 a. 删除 cloudera.navigator.client.config 配置项 b. 删除dfs.namenode.audit.loggers 配置项 c. 按如下格式编辑 dfs.hosts 配置项 12345# Original version of the dfs.hosts property:&lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt;&lt;value&gt;/var/run/cloudera-scm-agent/process/63-hdfs-NAMENODE/dfs_all_hosts.txt&lt;/value&gt;&lt;/property&gt; 12345# New version of the dfs.hosts property:&lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt;&lt;value&gt;/etc/hadoop/conf.rollback.namenode/dfs_all_hosts.txt&lt;/value&gt;&lt;/property&gt; 编辑/etc/hadoop/conf.rollback.namenode/core-site.xml文件，按如下格式修改net.topology.script.file.name配置项 12345# Original property&lt;property&gt;&lt;name&gt;net.topology.script.file.name&lt;/name&gt;&lt;value&gt;/var/run/cloudera-scm-agent/process/63-hdfs-NAMENODE/topology.py&lt;/value&gt;&lt;/property&gt; 按如下格式编辑/etc/hadoop/conf.rollback.namenode/topology.py文件的MAP_FILE项 1MAP_FILE = '/etc/hadoop/conf.rollback.namenode/topology.map' 执行下面命令回滚: 1sudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.namenode namenode -rollback 有可能报错建立目录失败，手动建立下日志目录，并修改下目录权限777，然后执行回滚，回滚会提示是否需要回滚，选择Y，最后提前启动Namenode失败，继续执行下面步骤 通过CM重启所有的NameNodes 和 JournalNodes 实例 回滚DataNode 编辑/etc/hadoop/conf.rollback.datanode/hdfs-site.xml 移除dfs.datanode.max.locked.memory属性 运行如下命令 12mkdir -p /opt/cloudera/parcels/CDH-6.0.0-1.cdh6.0.0.p0.537114/lib/hadoop/logschmod 777 /opt/cloudera/parcels/CDH-6.0.0-1.cdh6.0.0.p0.537114/lib/hadoop/logs 12cd /etc/hadoop/conf.rollback.datanodesudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.datanode datanode -rollback 回滚完成后通过CTRL + C返回终端 对于HA的HDFS，通过CM重启HDFS服务，非HA的HDFS重启所有的DataNode角色。 (非HA的HDFS回滚Secondary NameNode )If high availability is not enabled for HDFS, roll back the Secondary NameNode. (Clusters with TLS enabled only) Edit the/etc/hadoop/conf.rollback.secondarynamenode/ssl-server.xml file on all Secondary NameNode hosts (Located in the temporary rollback directory.) and update the keystore passwords with the actual cleartext passwords. The passwords will have values that look like this: 12345678&lt;property&gt; &lt;name&gt;ssl.server.keystore.password&lt;/name&gt; &lt;value&gt;********&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt; &lt;value&gt;********&lt;/value&gt; &lt;/property&gt; (TLS only) Edit the /etc/hadoop/conf.rollback.secondarynamenode/ssl-server.xml file and remove the hadoop.security.credential.provider.path property. Log in to the Secondary NameNode host and run the following commands: 123rm -rf /dfs/snn/*cd /etc/hadoop/conf.rollback.secondarynamenode/sudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.secondarynamenode secondarynamenode -format Restart the HDFS service. Open the Cloudera Manager Admin Console, go to the HDFS service page, and select Actions &gt; Restart. The Restart Command page displays the progress of the restart. Wait for the page to display the Successfully restarted service message before continuing. 启动HBase服务 通过CM直接启动HBase服务 回滚数据库 Hive Metastore Hue Oozie Sentry Server 12345678# 还原huemysqldump --databases hue --host=localhost --port=3306 -u hue -p &lt; hue-backup-`date +%F`-CDH6.0.0.sql# 还原hive元数据mysqldump --databases metastore --host=localhost --port=3306 -u hive -p &lt; metastore-backup-`date +%F`-CDH6.0.0.sql# 还原ooziemysqldump --databases oozie --host=localhost --port=3306 -u oozie -p &lt; oozie-backup-`date +%F`-CDH6.0.0.sql# 还原sentrymysqldump --databases sentry --host=localhost --port=3306 -u sentry -p &lt; sentry-backup-`date +%F`-CDH6.0.0.sql mysqldump恢复sentry库后，重启sentry失败，后来发现mysqldump没恢复成功，可以切换到sentry库下使用source恢复 启动Sentry服务 通过CM直接启动Sentry服务 回滚Hue 12rm -rf /opt/cloudera/parcels/CDH/lib/hue/app.regcp -rp /opt/cloudera/parcels_backup/app.reg-CM6.1.0-CDH6.0.0 /opt/cloudera/parcels/CDH/lib/hue/app.reg 重新部署客户端配置 重启集群 回滚CDH版本后，会导致Oozie的共享库版本与CDH版本不匹配问题，需要降级Oozie的共享库 3.4.检查服务和数据 Hue数据检查 升级前的数据 回滚后的数据 创建表 插入数据 至此CDH的回滚完成]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCDH6.0.0update6.1.0%2FCDH6.0.0%E5%8D%87%E7%BA%A76.1.0%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[CDH6.0.0升级6.1.0步骤官方文档 参考文档 [TOC] 1.CM升级1.1.升级概述 升级CDH集群，可以通过Cloudera Manager使用Cloudera parcels升级整个集群，也可以在所有群集主机使用基于RPM包的命令手动安装软件，然后Cloudera Manager完成服务升级。 可以不同时升级Cloudera Manager和CDH，但Cloudera Manager和CDH的版本必须兼容，Cloudera Manager的主要+次要版本等于或高于CDH的主要+次要版本。 1.2.下载文件CM6.1.0 北京IDC内网地址 1.3.备份官方文档 1.3.1.收集信息 操作系统信息 1lsb_release -a 查看数据库配置信息 1cat /etc/cloudera-scm-server/db.properties 123456789101112# cat /etc/cloudera-scm-server/db.properties# Auto-generated by scm_prepare_database.sh on Thu Dec 6 18:42:08 CST 2018## For information describing how to configure the Cloudera Manager Server# to connect to databases, see the "Cloudera Manager Installation Guide."#com.cloudera.cmf.db.type=mysqlcom.cloudera.cmf.db.host=localhostcom.cloudera.cmf.db.name=scmcom.cloudera.cmf.db.user=scmcom.cloudera.cmf.db.setupType=EXTERNALcom.cloudera.cmf.db.password=scm 1.3.2.备份CM Agent 创建备份目录 （在所有主机上执行） 123export CM_BACKUP_DIR="`date +%F`-CM6.0.0"echo $CM_BACKUP_DIRmkdir -p $CM_BACKUP_DIR The tar commands in the steps below may return the following message. It is safe to ignore thismessage: 12&gt; tar: Removing leading `/&apos; from member names&gt; 备份CM Agent当前运行状态（在所有主机上执行） 1tar -cf $CM_BACKUP_DIR/cloudera-scm-agent.tar --exclude=*.sock /etc/cloudera-scm-agent /etc/default/cloudera-scm-agent /var/run/cloudera-scm-agent /var/lib/cloudera-scm-agent 备份yum源（在所有主机上执行） 1tar -cf $CM_BACKUP_DIR/repository.tar /etc/yum.repos.d 1.3.3.备份Cloudera Management Service 在Service Monitor role 机器上执行 1cp -rp /var/lib/cloudera-service-monitor /var/lib/cloudera-service-monitor-`date +%F`-CM6.0.0 在Host Monitor role机器上执行 1cp -rp /var/lib/cloudera-host-monitor /var/lib/cloudera-host-monitor-`date +%F`-CM6.0.0 在 Event Server role机器上执行 1cp -rp /var/lib/cloudera-scm-eventserver /var/lib/cloudera-scm-eventserver-`date +%F`-CM6.0.0 1.3.4.停止CM相关服务 停止Cloudera Management Service 登录CM管理界面操作 停止Cloudera Manager Server （CM机器上执行） 1systemctl stop cloudera-scm-server 1.3.5.备份CM数据库（mysql机器上执行）1mysqldump --databases scm --host=localhost --port=3306 -u scm -p &gt; $HOME/scm-backup-`date +%F`-CM6.0.0.sql 1.3.6.备份CM其他相关服务的数据库（mysql机器上执行） 1mysqldump --databases am --host=localhost --port=3306 -u am -p &gt; $HOME/am-backup-`date +%F`-CM6.0.0.sql 1.3.7.备份Cloudera Manager Server（CM机器上执行）1tar -cf $CM_BACKUP_DIR/cloudera-scm-server.tar /etc/cloudera-scm-server /etc/default/cloudera-scm-server 1.4.升级Cloudera Manager Server 删除yum源 1rm /etc/yum.repos.d/cloudera*manager.repo* 添加新的yum源 1vim /etc/yum.repos.d/cloudera-manager.repo 12345[cloudera-repo]name=cloudera-repobaseurl=http://10.240.9.132:8900/CDH6.1.0/cm/cm6/6.1.0/redhat7/yum/enabled=1gpgcheck=0 123yum clean allrm -rf /var/cache/yumyum repolist 停止cloudera-scm-agent服务 1systemctl stop cloudera-scm-agent 升级CM 12yum clean allyum upgrade cloudera-manager-server cloudera-manager-daemons cloudera-manager-agent 如果弹出更新配置文件选项选择N You might be prompted about your configuration file version: 12345678910&gt; Configuration file &apos;/etc/cloudera-scm-agent/config.ini&apos;&gt; ==&gt; Modified (by you or by a script) since installation.&gt; ==&gt; Package distributor has shipped an updated version.&gt; What would you like to do about it ? Your options are:&gt; Y or I : install the package maintainer&apos;s version&gt; N or O : keep your currently-installed version&gt; D : show the differences between the versions&gt; Z : start a shell to examine the situation&gt; The default action is to keep your current version.&gt; You may receive a similar prompt for /etc/cloudera-scm-server/db.properties. Answer N to both prompts. You may be prompted to accept the GPG key. Answer y. 123456&gt; Retrieving key from https://archive.cloudera.com/.../cm/RPM-GPG-KEY-cloudera&gt; Importing GPG key ...&gt; Userid : &quot;Yum Maintainer &lt;webmaster@cloudera.com&gt;&quot;&gt; Fingerprint: ...&gt; From : https://archive.cloudera.com/.../RPM-GPG-KEY-cloudera&gt; 检查安装的包是否正确 1rpm -qa 'cloudera-manager-*' 123cloudera-manager-server-6.1.0-769885.el7.x86_64cloudera-manager-daemons-6.1.0-769885.el7.x86_64cloudera-manager-agent-6.1.0-769885.el7.x86_64 启动服务 12systemctl start cloudera-scm-agentsystemctl start cloudera-scm-server 检查日志 12tail -f /var/log/cloudera-scm-server/cloudera-scm-server.logtail -f /var/log/cloudera-scm-agent/cloudera-scm-agent.log 登录CM管理平台http://10.240.9.132:7180自动运行升级向导 输入升级源 http://10.240.9.132:8900/CDH6.1.0/cm/cm6/6.1.0/ Cloudera Manager在升级后报告过时的配置，请重新启动集群服务并重新部署客户端配置 检查升级结果 查看Cloudera Manager版本 验证Agent是否向Cloudera Manager发送心跳 检查所有主机 查看检查结果 检查集群历史监控数据 至此CM升级成功。 2.CDH升级官方文档 2.1.准备安装包官方CDH6.1.0下载地址 北京IDC内网地址 2.2.服务检查 HDFS检查 123# 登录任意一个节点，检查HDFS是否有错误，有错误修改后继续sudo -u hdfs hdfs fsck / -includeSnapshotssudo -u hdfs hdfs dfsadmin -report HBase检查 12# 登录任意一个DataNode节点，执行如下命令检查，有错误的话，修正错误后继续sudo -u hdfs hbase hbck KUDU检查 123# 执行如下命令检查，有错误的话，修正错误后继续# kudu kudu cluster ksck kudu_master_host1,ksck kudu_master_host2sudo -u kudu kudu cluster ksck bjds-bi-odp-prd-10-240-9-132-belle.lan 2.3.停止服务 2.4.备份数据库12345678# 备份huemysqldump --databases hue --host=localhost --port=3306 -u hue -p &gt; hue-backup-`date +%F`-CDH6.0.0.sql# 备份hive元数据mysqldump --databases metastore --host=localhost --port=3306 -u hive -p &gt; metastore-backup-`date +%F`-CDH6.0.0.sql# 备份ooziemysqldump --databases oozie --host=localhost --port=3306 -u oozie -p &gt; oozie-backup-`date +%F`-CDH6.0.0.sql# 备份sentrymysqldump --databases sentry --host=localhost --port=3306 -u sentry -p &gt; sentry-backup-`date +%F`-CDH6.0.0.sql 2.5.备份ZooKeeper(所有节点)1cp -rp /var/lib/zookeeper/ /var/lib/zookeeper-backup-`date +%F`CM6.1.0-CDH6.0.0 2.6.备份数据 启用了HA的HDFS需要登录所有的JournalNode节点备份 1cp -rp /data/dfs/jn /data/dfs/jn-CM6.1.0-CDH6.0.0 备份所有的NameNode节点运行状态 12345mkdir -p /etc/hadoop/conf.rollback.namenodecd /var/run/cloudera-scm-agent/process/ &amp;&amp; cd `ls -t1 | grep -e "-NAMENODE\$" | head -1`cp -rp * /etc/hadoop/conf.rollback.namenode/rm -rf /etc/hadoop/conf.rollback.namenode/log4j.propertiescp -rp /etc/hadoop/conf.cloudera.hdfs/log4j.properties /etc/hadoop/conf.rollback.namenode/ 备份NameNode的元数据（所有namenode主机） 1tar -czvf $CM_BACKUP_DIR/nn_backup.tar.gz /data/dfs/nn/ 备份所有的DataNode运行状态（所有运行DataNode主机） 12345mkdir -p /etc/hadoop/conf.rollback.datanode/cd /var/run/cloudera-scm-agent/process/ &amp;&amp; cd `ls -t1 | grep -e "-DATANODE\$" | head -1`cp -rp * /etc/hadoop/conf.rollback.datanode/rm -rf /etc/hadoop/conf.rollback.datanode/log4j.propertiescp -rp /etc/hadoop/conf.cloudera.hdfs/log4j.properties /etc/hadoop/conf.rollback.datanode/ 备份Hue，所有的Hue Server主机 123# 在所有的Hue Server角色上运行mkdir -p /opt/cloudera/parcels_backupcp -rp /opt/cloudera/parcels/CDH/lib/hue/app.reg /opt/cloudera/parcels_backup/app.reg-CM6.1.0-CDH6.0.0 2.7.向集群中添加新版的CDH存储库 http://10.240.9.132:8900/CDH6.1.0/cdh/ 2.8.运行升级向导开始升级 2.9.功能验证 运行一个MR任务 1sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 100 Hue测试 检查原有数据 CDH6.0.0的数据 升级CDH6.1.0后的数据 创建数据库 创建表和插入数据 2.10.最终化元数据升级在最终化元数据之前，进行几天甚至几周的运行观察集群是否正常，在发现所有任务都没有任何异常情况后，再进行最终化元数据操作。一旦进行最终化元数据之后，就不能回滚到老的版本了，除非有数据备份。对NameNode的主备节点都执行最终化元数据升级操作： 3.CDH回滚官方文档 参考文档 3.1.回滚限制 HDFS - 不能对进行过最终化元数据升级（2.10步骤）的HDFS集群进行回滚。 集群升级后的配置变化将不会被保留 - 包括升级后的角色、服务的变更。Cloudera 的建议是最终化元数据升级后再进行集群角色和服务的变更。 HBase – If your cluster is configured to use HBase replication, data written to HBase after the upgrade might not be replicated to peers when you start your rollback. This topic does not describe how to determine which, if any, peers have the replicated data and how to roll back that data. For more information about HBase replication, see HBase Replication. Sqoop 1 – Because of the changes introduced in Sqoop metastore logic, the metastore database that is created by the CDH 6.x version of Sqoop cannot be used by earlier versions. Sqoop 2 – As described in the upgrade process, Sqoop2 had to be stopped and deleted before the upgrade process and therefore will not be available after the rollback. Kafka – Once the Kafka log format and protocol version configurations (the inter.broker.protocol.version and log.message.format.version properties) are set to the new version (or left blank, which means to use the latest version), Kafka rollback is not possible. 3.2.停止服务 3.3.回滚CDH 回滚Parcel 点击激活会弹出对话框，选择第二个选项，仅激活，如果不小心选择了第一个选项，开始下面步骤前，先停止服务。 还原ZooKeeper（所有主机） 12rm -fr /var/lib/zookeeper/cp -rp /var/lib/zookeeper-backup-`date +%F`CM6.1.0-CDH6.0.0 /var/lib/zookeeper 通过CM启动zookeeper 回滚HDFS 对于开启了HA的HDFS回滚Journal Nodes 登录每个Journal Node节点主机，运行下面命令 1rm -fr /data/dfs/jn/nameservice1/current/* 1cp -rp /data/dfs/jn-CM6.1.0-CDH6.0.0/nameservice1/current/* /data/dfs/jn/nameservice1/current/ 通过CM启动JournalNodes角色 回滚 NameNode（NameNode角色主机执行） 编辑/etc/hadoop/conf.rollback.namenode/hdfs-site.xml文件 a. 删除 cloudera.navigator.client.config 配置项 b. 删除dfs.namenode.audit.loggers 配置项 c. 按如下格式编辑 dfs.hosts 配置项 12345# Original version of the dfs.hosts property:&lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt;&lt;value&gt;/var/run/cloudera-scm-agent/process/63-hdfs-NAMENODE/dfs_all_hosts.txt&lt;/value&gt;&lt;/property&gt; 12345# New version of the dfs.hosts property:&lt;property&gt;&lt;name&gt;dfs.hosts&lt;/name&gt;&lt;value&gt;/etc/hadoop/conf.rollback.namenode/dfs_all_hosts.txt&lt;/value&gt;&lt;/property&gt; 编辑/etc/hadoop/conf.rollback.namenode/core-site.xml文件，按如下格式修改net.topology.script.file.name配置项 12345# Original property&lt;property&gt;&lt;name&gt;net.topology.script.file.name&lt;/name&gt;&lt;value&gt;/var/run/cloudera-scm-agent/process/63-hdfs-NAMENODE/topology.py&lt;/value&gt;&lt;/property&gt; 按如下格式编辑/etc/hadoop/conf.rollback.namenode/topology.py文件的MAP_FILE项 1MAP_FILE = '/etc/hadoop/conf.rollback.namenode/topology.map' 执行下面命令回滚: 1sudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.namenode namenode -rollback 有可能报错建立目录失败，手动建立下日志目录，并修改下目录权限777，然后执行回滚，回滚会提示是否需要回滚，选择Y，最后提前启动Namenode失败，继续执行下面步骤 通过CM重启所有的NameNodes 和 JournalNodes 实例 回滚DataNode 编辑/etc/hadoop/conf.rollback.datanode/hdfs-site.xml 移除dfs.datanode.max.locked.memory属性 运行如下命令 12mkdir -p /opt/cloudera/parcels/CDH-6.0.0-1.cdh6.0.0.p0.537114/lib/hadoop/logschmod 777 /opt/cloudera/parcels/CDH-6.0.0-1.cdh6.0.0.p0.537114/lib/hadoop/logs 12cd /etc/hadoop/conf.rollback.datanodesudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.datanode datanode -rollback 回滚完成后通过CTRL + C返回终端 对于HA的HDFS，通过CM重启HDFS服务，非HA的HDFS重启所有的DataNode角色。 (非HA的HDFS回滚Secondary NameNode )If high availability is not enabled for HDFS, roll back the Secondary NameNode. (Clusters with TLS enabled only) Edit the/etc/hadoop/conf.rollback.secondarynamenode/ssl-server.xml file on all Secondary NameNode hosts (Located in the temporary rollback directory.) and update the keystore passwords with the actual cleartext passwords. The passwords will have values that look like this: 12345678&lt;property&gt; &lt;name&gt;ssl.server.keystore.password&lt;/name&gt; &lt;value&gt;********&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ssl.server.keystore.keypassword&lt;/name&gt; &lt;value&gt;********&lt;/value&gt; &lt;/property&gt; (TLS only) Edit the /etc/hadoop/conf.rollback.secondarynamenode/ssl-server.xml file and remove the hadoop.security.credential.provider.path property. Log in to the Secondary NameNode host and run the following commands: 123rm -rf /dfs/snn/*cd /etc/hadoop/conf.rollback.secondarynamenode/sudo -u hdfs hdfs --config /etc/hadoop/conf.rollback.secondarynamenode secondarynamenode -format Restart the HDFS service. Open the Cloudera Manager Admin Console, go to the HDFS service page, and select Actions &gt; Restart. The Restart Command page displays the progress of the restart. Wait for the page to display the Successfully restarted service message before continuing. 启动HBase服务 通过CM直接启动HBase服务 回滚数据库 Hive Metastore Hue Oozie Sentry Server 12345678# 还原huemysqldump --databases hue --host=localhost --port=3306 -u hue -p &lt; hue-backup-`date +%F`-CDH6.0.0.sql# 还原hive元数据mysqldump --databases metastore --host=localhost --port=3306 -u hive -p &lt; metastore-backup-`date +%F`-CDH6.0.0.sql# 还原ooziemysqldump --databases oozie --host=localhost --port=3306 -u oozie -p &lt; oozie-backup-`date +%F`-CDH6.0.0.sql# 还原sentrymysqldump --databases sentry --host=localhost --port=3306 -u sentry -p &lt; sentry-backup-`date +%F`-CDH6.0.0.sql mysqldump恢复sentry库后，重启sentry失败，后来发现mysqldump没恢复成功，可以切换到sentry库下使用source恢复 启动Sentry服务 通过CM直接启动Sentry服务 回滚Hue 12rm -rf /opt/cloudera/parcels/CDH/lib/hue/app.regcp -rp /opt/cloudera/parcels_backup/app.reg-CM6.1.0-CDH6.0.0 /opt/cloudera/parcels/CDH/lib/hue/app.reg 重新部署客户端配置 重启集群 回滚CDH版本后，会导致Oozie的共享库版本与CDH版本不匹配问题，需要降级Oozie的共享库 3.4.检查服务和数据 Hue数据检查 升级前的数据 回滚后的数据 创建表 插入数据 至此CDH的回滚完成]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F16%2FCDH5.15%E4%B8%AD%E6%B7%BB%E5%8A%A0StreamSets%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[CDH中添加StreamSets服务1.安装准备下载安装包：https://archives.streamsets.com/index.html 下载如下三个文件： manifest.json Cloudera Parcels RHEL 7 ( SHA) (下载对应操作系统版本)（比较大，约4G左右，国内网速慢，建议使用代理） Custom Service Descriptor (CSD) 2.配置本地Parcels文件服务1234cd softpython -m SimpleHTTPServer 8900Serving HTTP on 0.0.0.0 port 8900 ... 3.配置CSD1234# 将STREAMSETS-3.5.1.jar拷贝到/opt/cloudera/csd,并更改权限，然后重启cloudera-scm-server服务cp STREAMSETS-3.5.1.jar /opt/cloudera/csdchown cloudera-scm:cloudera-scm STREAMSETS-3.5.1.jar &amp;&amp; chmod 644 STREAMSETS-3.5.1.jar/opt/cm-5.15.1/etc/init.d/cloudera-scm-server restart 4.下载分发激活Parcel包在CM界面中点击Parcel &gt; 配置 &gt; 添加StreamSets的Parcel包路径，并保持修改 下载分发和激活(本文档省略该步骤，直接查看激活后的状态) 5.添加StreamSets服务 6.登录 默认的账户: admin 密码:admin 至此StreamSets服务安装完毕]]></content>
  </entry>
</search>
